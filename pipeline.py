"""
================================================================================
                        ETL & TRAINING PIPELINE
================================================================================

DESCRIPTION:
    Ce script est le c≈ìur du pipeline de donn√©es. Il s'ex√©cute une seule fois
    au d√©marrage du projet (dans un container Docker) et effectue 3 t√¢ches :

    1. EXTRACT : Lit les donn√©es brutes depuis le fichier CSV
    2. LOAD    : Envoie les donn√©es dans PostgreSQL
    3. TRAIN   : Entra√Æne un mod√®le ML et le sauvegarde

FLUX D'EX√âCUTION:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   iris.csv      ‚îÇ  ‚Üê Fichier source (150 fleurs Iris)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ    EXTRACT      ‚îÇ  ‚Üê Lecture avec Pandas
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ     LOAD        ‚îÇ  ‚Üê Insertion dans PostgreSQL
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ     TRAIN       ‚îÇ  ‚Üê Entra√Ænement RandomForest
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  iris_model.joblib  ‚îÇ  ‚Üê Mod√®le sauvegard√© (utilis√© par l'API)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

D√âPENDANCES:
    - pandas       : Manipulation des donn√©es
    - sqlalchemy   : Connexion √† PostgreSQL
    - scikit-learn : Algorithme de Machine Learning
    - mlflow       : Tracking des exp√©riences
    - joblib       : S√©rialisation du mod√®le

AUTEUR: Projet Data Pipeline - Epitech 2025-2026
================================================================================
"""

# ==============================================================================
# IMPORTS
# ==============================================================================

import os          # Acc√®s aux variables d'environnement et syst√®me de fichiers
import time        # Pour attendre entre les tentatives de connexion
import pandas as pd   # Manipulation de donn√©es (DataFrames)
import numpy as np    # Calculs num√©riques
from sqlalchemy import create_engine, text  # Connexion et requ√™tes PostgreSQL
import mlflow         # Plateforme de tracking ML
import mlflow.sklearn # Extension MLflow pour scikit-learn
from sklearn.model_selection import train_test_split  # S√©paration train/test
from sklearn.ensemble import RandomForestRegressor    # Algorithme de ML
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # M√©triques
import joblib  # Sauvegarde/chargement du mod√®le


# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Les valeurs sont r√©cup√©r√©es depuis les variables d'environnement Docker.
# Si non d√©finies, des valeurs par d√©faut sont utilis√©es pour le dev local.

# --- Configuration PostgreSQL ---
POSTGRES_USER = os.getenv("POSTGRES_USER", "admin")        # Utilisateur BDD
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "admin") # Mot de passe BDD
POSTGRES_HOST = os.getenv("POSTGRES_HOST", "db")           # Nom du service Docker
POSTGRES_PORT = os.getenv("POSTGRES_PORT", "5432")         # Port PostgreSQL
POSTGRES_DB = os.getenv("POSTGRES_DB", "datapipeline")     # Nom de la base

# --- Configuration MLflow ---
# URL du serveur MLflow pour le tracking des exp√©riences
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://mlflow:5000")

# --- URL de connexion PostgreSQL ---
# Format: postgresql://user:password@host:port/database
DATABASE_URL = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"


# ==============================================================================
# FONCTIONS D'ATTENTE DES SERVICES
# ==============================================================================
# Ces fonctions sont essentielles car Docker ne garantit pas l'ordre de d√©marrage.
# Le pipeline doit attendre que PostgreSQL et MLflow soient pr√™ts avant de continuer.

def wait_for_db(max_retries: int = 30, delay: int = 2) -> bool:
    """
    Attend que PostgreSQL soit pr√™t √† recevoir des connexions.

    POURQUOI C'EST N√âCESSAIRE:
        Docker d√©marre les containers en parall√®le. M√™me si PostgreSQL d√©marre
        avant le pipeline, il lui faut quelques secondes pour √™tre op√©rationnel.
        Sans cette attente, le pipeline √©chouerait imm√©diatement.

    FONCTIONNEMENT:
        1. Tente de se connecter √† PostgreSQL
        2. Ex√©cute une requ√™te simple "SELECT 1" pour v√©rifier
        3. Si √©chec, attend 'delay' secondes et r√©essaie
        4. Abandonne apr√®s 'max_retries' tentatives

    Args:
        max_retries: Nombre maximum de tentatives (d√©faut: 30)
        delay: D√©lai entre chaque tentative en secondes (d√©faut: 2)

    Returns:
        True si la connexion r√©ussit, False sinon
    """
    print(f"‚è≥ Attente de PostgreSQL ({POSTGRES_HOST}:{POSTGRES_PORT})...")

    for i in range(max_retries):
        try:
            # Cr√©ation d'une connexion temporaire
            engine = create_engine(DATABASE_URL)
            with engine.connect() as conn:
                # Requ√™te simple pour tester la connexion
                conn.execute(text("SELECT 1"))
            print("   ‚úÖ PostgreSQL est pr√™t!")
            engine.dispose()  # Lib√®re la connexion
            return True
        except Exception as e:
            # La connexion a √©chou√©, on attend et on r√©essaie
            print(f"   ‚è≥ Tentative {i+1}/{max_retries}...")
            time.sleep(delay)

    print("   ‚ùå PostgreSQL n'est pas disponible")
    return False


def wait_for_mlflow(max_retries: int = 30, delay: int = 2) -> bool:
    """
    Attend que le serveur MLflow soit pr√™t.

    POURQUOI C'EST N√âCESSAIRE:
        MLflow doit √™tre op√©rationnel pour enregistrer les m√©triques et le mod√®le.
        On v√©rifie sa disponibilit√© via son endpoint /health.

    FONCTIONNEMENT:
        1. Envoie une requ√™te HTTP GET √† l'endpoint /health de MLflow
        2. Si r√©ponse OK (200), MLflow est pr√™t
        3. Sinon, attend et r√©essaie

    Args:
        max_retries: Nombre maximum de tentatives
        delay: D√©lai entre chaque tentative en secondes

    Returns:
        True si MLflow r√©pond, False sinon
    """
    import urllib.request
    import urllib.error

    print(f"‚è≥ Attente de MLflow ({MLFLOW_TRACKING_URI})...")

    for i in range(max_retries):
        try:
            # Requ√™te HTTP simple vers l'endpoint de sant√©
            urllib.request.urlopen(f"{MLFLOW_TRACKING_URI}/health", timeout=5)
            print("   ‚úÖ MLflow est pr√™t!")
            return True
        except (urllib.error.URLError, Exception):
            print(f"   ‚è≥ Tentative {i+1}/{max_retries}...")
            time.sleep(delay)

    print("   ‚ùå MLflow n'est pas disponible")
    return False


# ==============================================================================
# √âTAPE 1: EXTRACT (EXTRACTION DES DONN√âES)
# ==============================================================================

def extract_data(filepath: str) -> pd.DataFrame:
    """
    Lit les donn√©es depuis un fichier CSV et les charge dans un DataFrame.

    C'EST QUOI UN DATAFRAME:
        Un DataFrame est une structure de donn√©es tabulaire (comme un tableau Excel)
        fournie par la librairie Pandas. Chaque colonne a un nom et un type.

    CONTENU DU FICHIER iris.csv:
        - sepal_length : Longueur des s√©pales (cm) ‚Üê CE QU'ON VEUT PR√âDIRE
        - sepal_width  : Largeur des s√©pales (cm)  ‚Üê NOTRE VARIABLE D'ENTR√âE
        - petal_length : Longueur des p√©tales (cm)
        - petal_width  : Largeur des p√©tales (cm)
        - species      : Esp√®ce de la fleur (setosa, versicolor, virginica)

    Args:
        filepath: Chemin vers le fichier CSV

    Returns:
        DataFrame Pandas contenant les donn√©es
    """
    print(f"üì• Lecture du fichier: {filepath}")

    # Lecture du CSV - Pandas d√©tecte automatiquement les colonnes et types
    df = pd.read_csv(filepath)

    # Affichage des informations sur les donn√©es charg√©es
    print(f"   ‚úÖ {len(df)} lignes charg√©es")
    print(f"   üìä Colonnes: {list(df.columns)}")

    return df


# ==============================================================================
# √âTAPE 2: LOAD (CHARGEMENT DANS POSTGRESQL)
# ==============================================================================

def load_to_postgres(df: pd.DataFrame, table_name: str = "iris_data") -> None:
    """
    Envoie le DataFrame dans une table PostgreSQL.

    POURQUOI STOCKER DANS UNE BASE DE DONN√âES:
        1. Persistance : Les donn√©es survivent aux red√©marrages
        2. Requ√™tage : On peut interroger les donn√©es avec SQL
        3. Int√©gration : D'autres services peuvent acc√©der aux donn√©es
        4. Historique : On garde une trace des donn√©es d'entra√Ænement

    FONCTIONNEMENT:
        1. Cr√©e une connexion √† PostgreSQL via SQLAlchemy
        2. Utilise df.to_sql() pour cr√©er la table et ins√©rer les donn√©es
        3. if_exists="replace" : Supprime et recr√©e la table √† chaque ex√©cution

    Args:
        df: DataFrame contenant les donn√©es √† ins√©rer
        table_name: Nom de la table de destination (d√©faut: "iris_data")
    """
    print(f"üì§ Envoi des donn√©es vers PostgreSQL (table: {table_name})")

    # Cr√©ation de la connexion
    engine = create_engine(DATABASE_URL)

    # Insertion des donn√©es
    # if_exists="replace" : Si la table existe, on la supprime et on la recr√©e
    # index=False : On n'ins√®re pas l'index du DataFrame comme colonne
    df.to_sql(table_name, engine, if_exists="replace", index=False)

    # V√©rification : On compte les lignes ins√©r√©es
    with engine.connect() as conn:
        result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
        count = result.scalar()

    print(f"   ‚úÖ {count} lignes ins√©r√©es dans la table '{table_name}'")

    # Lib√©ration de la connexion
    engine.dispose()


# ==============================================================================
# √âTAPE 3: TRAIN (ENTRA√éNEMENT DU MOD√àLE)
# ==============================================================================

def train_model(df: pd.DataFrame) -> dict:
    """
    Entra√Æne un mod√®le RandomForestRegressor sur les donn√©es Iris.

    OBJECTIF:
        Pr√©dire sepal_length (longueur des s√©pales) √† partir de sepal_width (largeur)
        C'est un probl√®me de R√âGRESSION (pr√©dire une valeur continue, pas une cat√©gorie)

    ALGORITHME UTILIS√â - RandomForest:
        - Ensemble de 100 arbres de d√©cision
        - Chaque arbre apprend sur un √©chantillon diff√©rent des donn√©es
        - La pr√©diction finale = moyenne des pr√©dictions de tous les arbres
        - Avantages : Robuste, peu de r√©glages n√©cessaires, bon pour les petits datasets

    M√âTRIQUES CALCUL√âES:
        - RMSE (Root Mean Square Error) : Erreur quadratique moyenne, p√©nalise les grandes erreurs
        - MAE (Mean Absolute Error) : Erreur absolue moyenne, plus intuitive
        - R¬≤ Score : Coefficient de d√©termination, mesure la qualit√© de l'ajustement (1 = parfait)

    MLFLOW TRACKING:
        Tout est enregistr√© dans MLflow pour le suivi :
        - Param√®tres du mod√®le (n_estimators, max_depth...)
        - M√©triques de performance (RMSE, MAE, R¬≤)
        - Le mod√®le lui-m√™me (pour le r√©cup√©rer plus tard)

    Args:
        df: DataFrame avec les donn√©es d'entra√Ænement

    Returns:
        Dictionnaire contenant les m√©triques calcul√©es
    """
    print("ü§ñ Entra√Ænement du mod√®le (R√©gression)...")

    # --- Configuration de MLflow ---
    # On dit √† MLflow o√π envoyer les donn√©es (serveur MLflow dans Docker)
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    # Nom de l'exp√©rience (groupe de "runs" li√©s)
    mlflow.set_experiment("iris-regression")

    # --- Pr√©paration des donn√©es ---
    # X = Features (variables d'entr√©e) - ici juste sepal_width
    # y = Target (variable √† pr√©dire) - ici sepal_length
    # .values convertit en array NumPy (format attendu par scikit-learn)
    X = df[["sepal_width"]].values  # Double crochets = DataFrame 2D ‚Üí Array 2D
    y = df["sepal_length"].values   # Simple crochets = Series 1D ‚Üí Array 1D

    # --- S√©paration Train/Test ---
    # 80% des donn√©es pour l'entra√Ænement, 20% pour le test
    # random_state=42 : Graine al√©atoire fixe pour reproductibilit√©
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # --- D√©finition des hyperparam√®tres ---
    # Ce sont les "r√©glages" de l'algorithme
    params = {
        "n_estimators": 100,      # Nombre d'arbres dans la for√™t
        "max_depth": 5,           # Profondeur max de chaque arbre (√©vite le surapprentissage)
        "min_samples_split": 2,   # Min √©chantillons pour diviser un n≈ìud
        "min_samples_leaf": 1,    # Min √©chantillons dans une feuille
        "random_state": 42        # Reproductibilit√©
    }

    # --- Entra√Ænement avec tracking MLflow ---
    # Un "run" = une ex√©cution d'entra√Ænement avec ses param√®tres et r√©sultats
    with mlflow.start_run(run_name="random_forest_regression"):

        # 1) Logger les param√®tres (pour pouvoir les retrouver plus tard)
        mlflow.log_params(params)
        mlflow.log_param("feature", "sepal_width")
        mlflow.log_param("target", "sepal_length")

        # 2) Cr√©er et entra√Æner le mod√®le
        # **params = d√©compresse le dictionnaire en arguments nomm√©s
        model = RandomForestRegressor(**params)
        model.fit(X_train, y_train)  # L'entra√Ænement proprement dit

        # 3) √âvaluer le mod√®le sur les donn√©es de test
        y_pred = model.predict(X_test)

        # 4) Calculer les m√©triques
        metrics = {
            "rmse": np.sqrt(mean_squared_error(y_test, y_pred)),  # Erreur quadratique
            "mae": mean_absolute_error(y_test, y_pred),           # Erreur absolue
            "r2_score": r2_score(y_test, y_pred)                  # Coefficient R¬≤
        }

        # 5) Logger les m√©triques dans MLflow
        mlflow.log_metrics(metrics)

        # 6) Sauvegarder le mod√®le localement (pour l'API)
        # Ce fichier sera accessible par l'API via un volume Docker partag√©
        model_dir = "/app/models"
        os.makedirs(model_dir, exist_ok=True)  # Cr√©e le dossier s'il n'existe pas
        model_path = os.path.join(model_dir, "iris_model.joblib")
        joblib.dump(model, model_path)  # S√©rialisation du mod√®le

        # 7) Logger le chemin et le mod√®le dans MLflow
        mlflow.log_param("model_path", model_path)
        mlflow.sklearn.log_model(model, "model", registered_model_name="IrisModel")

        # 8) R√©cup√©rer l'ID du run pour r√©f√©rence
        run_id = mlflow.active_run().info.run_id

        # --- Affichage des r√©sultats ---
        print(f"   ‚úÖ Mod√®le entra√Æn√© avec succ√®s!")
        print(f"   üìä M√©triques:")
        for name, value in metrics.items():
            print(f"      - {name}: {value:.4f}")
        print(f"   üíæ Mod√®le sauvegard√©: {model_path}")
        print(f"   üîó MLflow Run ID: {run_id}")

    return metrics


# ==============================================================================
# FONCTION PRINCIPALE
# ==============================================================================

def main():
    """
    Point d'entr√©e du pipeline - Orchestre toutes les √©tapes.

    S√âQUENCE D'EX√âCUTION:
        1. Attendre que PostgreSQL soit pr√™t
        2. Attendre que MLflow soit pr√™t
        3. V√©rifier que le fichier CSV existe
        4. Extraire les donn√©es (EXTRACT)
        5. Charger dans PostgreSQL (LOAD)
        6. Entra√Æner le mod√®le (TRAIN)

    GESTION DES ERREURS:
        Si une √©tape √©choue, le script s'arr√™te avec exit(1)
        Docker d√©tectera l'√©chec et pourra relancer le container
    """
    print("=" * 50)
    print("üöÄ D√©marrage du pipeline ETL & Training")
    print("=" * 50)

    # --- √âtape pr√©liminaire : Attendre les d√©pendances ---
    if not wait_for_db():
        print("‚ùå Impossible de se connecter √† PostgreSQL. Abandon.")
        exit(1)  # Code de sortie 1 = erreur

    if not wait_for_mlflow():
        print("‚ùå Impossible de se connecter √† MLflow. Abandon.")
        exit(1)

    # --- V√©rification du fichier source ---
    csv_path = "/app/data/iris.csv"  # Chemin dans le container Docker

    if not os.path.exists(csv_path):
        print(f"‚ùå Fichier non trouv√©: {csv_path}")
        exit(1)

    # --- Ex√©cution du pipeline ETL ---

    # 1. EXTRACT : Lecture du CSV
    df = extract_data(csv_path)

    # 2. LOAD : Insertion dans PostgreSQL
    load_to_postgres(df)

    # 3. TRAIN : Entra√Ænement et sauvegarde du mod√®le
    metrics = train_model(df)

    print("=" * 50)
    print("‚úÖ Pipeline termin√© avec succ√®s!")
    print("=" * 50)


# ==============================================================================
# POINT D'ENTR√âE
# ==============================================================================
# Cette condition v√©rifie si le script est ex√©cut√© directement (pas import√©)

if __name__ == "__main__":
    main()
