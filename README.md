# üå∏ Iris Data Pipeline

Pipeline de donn√©es complet avec **4 services Docker** ind√©pendants pour l'entra√Ænement et le d√©ploiement d'un mod√®le de Machine Learning sur le dataset Iris.

## üìã Description

Ce projet impl√©mente un pipeline ETL (Extract, Transform, Load) qui :

1. **Charge** les donn√©es Iris depuis un fichier CSV
2. **Stocke** les donn√©es dans PostgreSQL
3. **Entra√Æne** un mod√®le de r√©gression (RandomForest) pour pr√©dire la longueur des s√©pales
4. **Expose** une API REST pour faire des pr√©dictions

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Docker Compose Network                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ  PostgreSQL  ‚îÇ    ‚îÇ    MLflow    ‚îÇ    ‚îÇ   FastAPI    ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ    (db)      ‚îÇ    ‚îÇ   (mlflow)   ‚îÇ    ‚îÇ    (api)     ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  Port: 5432  ‚îÇ    ‚îÇ  Port: 5001  ‚îÇ    ‚îÇ  Port: 8000  ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ         ‚ñ≤                   ‚ñ≤                   ‚ñ≤                ‚îÇ
‚îÇ         ‚îÇ                   ‚îÇ                   ‚îÇ                ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                             ‚îÇ                                    ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
‚îÇ                    ‚îÇ    Pipeline     ‚îÇ                          ‚îÇ
‚îÇ                    ‚îÇ  (pipeline_etl) ‚îÇ                          ‚îÇ
‚îÇ                    ‚îÇ   ETL + Train   ‚îÇ                          ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Structure du projet

```
datapipeline/
‚îú‚îÄ‚îÄ docker-compose.yml   # Orchestration des 4 services
‚îú‚îÄ‚îÄ Dockerfile           # Image Python pour pipeline et API
‚îú‚îÄ‚îÄ requirements.txt     # D√©pendances Python
‚îú‚îÄ‚îÄ pipeline.py          # Script ETL + Training
‚îú‚îÄ‚îÄ README.md            # Ce fichier
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ app.py           # API FastAPI
‚îî‚îÄ‚îÄ data/
    ‚îî‚îÄ‚îÄ iris.csv         # Dataset Iris
```

## üöÄ D√©marrage rapide

### Pr√©requis

- [Docker](https://www.docker.com/get-started) install√©
- [Docker Compose](https://docs.docker.com/compose/) install√©

### Lancer le projet

```bash
# Cloner le projet (si n√©cessaire)
cd datapipeline

# Lancer tous les services
docker compose up -d --build

# V√©rifier que tout fonctionne
docker compose ps
```

### R√©sultat attendu

```
NAME              IMAGE                           STATUS                    PORTS
api_service       datapipeline-api                Up                        0.0.0.0:8000->8000
mlflow_tracking   ghcr.io/mlflow/mlflow:v2.10.0   Up                        0.0.0.0:5001->5000
postgres_db       postgres:15-alpine              Up (healthy)              0.0.0.0:5432->5432
```

## üîÆ Faire une pr√©diction

### Option 1 : Via curl (Terminal)

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"sepal_width": 3.5}'
```

**R√©ponse :**

```json
{
  "sepal_width": 3.5,
  "predicted_sepal_length": 5.0815,
  "model_path": "/app/models/iris_model.joblib"
}
```

### Option 2 : Via Python

```python
import requests

response = requests.post(
    "http://localhost:8000/predict",
    json={"sepal_width": 3.5}
)

print(response.json())
# {'sepal_width': 3.5, 'predicted_sepal_length': 5.0815, ...}
```

### Option 3 : Via l'interface Swagger

1. Ouvrir http://localhost:8000/docs dans votre navigateur
2. Cliquer sur **POST /predict**
3. Cliquer sur **Try it out**
4. Entrer une valeur pour `sepal_width` (ex: 3.5)
5. Cliquer sur **Execute**

## üåê Interfaces Web

| Service         | URL                        | Description                        |
| --------------- | -------------------------- | ---------------------------------- |
| **API Swagger** | http://localhost:8000/docs | Documentation interactive de l'API |
| **MLflow UI**   | http://localhost:5001      | Suivi des exp√©riences ML           |

## üìä Endpoints de l'API

| M√©thode | Endpoint      | Description                |
| ------- | ------------- | -------------------------- |
| `GET`   | `/`           | Page d'accueil             |
| `GET`   | `/health`     | Statut de sant√© de l'API   |
| `GET`   | `/model/info` | Informations sur le mod√®le |
| `POST`  | `/predict`    | Faire une pr√©diction       |

### Exemple de requ√™te `/predict`

**Request:**

```json
{
  "sepal_width": 3.5
}
```

**Response:**

```json
{
  "sepal_width": 3.5,
  "predicted_sepal_length": 5.0815,
  "model_path": "/app/models/iris_model.joblib"
}
```

## üîß Commandes utiles

```bash
# Voir les logs de tous les services
docker compose logs -f

# Voir les logs d'un service sp√©cifique
docker compose logs -f api
docker compose logs -f pipeline

# Relancer uniquement le pipeline (r√©-entra√Æner le mod√®le)
docker compose up pipeline

# Arr√™ter tous les services
docker compose down

# Arr√™ter et supprimer les donn√©es (volumes)
docker compose down -v

# Reconstruire les images apr√®s modification du code
docker compose up -d --build
```

## üóÑÔ∏è Acc√©der √† PostgreSQL

```bash
# Se connecter √† la base de donn√©es
docker exec -it postgres_db psql -U admin -d datapipeline

# Voir les donn√©es
SELECT * FROM iris_data LIMIT 5;

# Compter les lignes
SELECT COUNT(*) FROM iris_data;

# Quitter
\q
```

## üìà Le mod√®le

- **Type** : RandomForestRegressor
- **Feature** : `sepal_width` (largeur des s√©pales)
- **Target** : `sepal_length` (longueur des s√©pales)
- **M√©triques** :
  - RMSE : ~0.85
  - MAE : ~0.65

## üêõ D√©pannage

### Le pipeline ne d√©marre pas

```bash
# V√©rifier les logs du pipeline
docker compose logs pipeline

# S'assurer que PostgreSQL est pr√™t
docker compose logs db
```

### L'API ne r√©pond pas

```bash
# V√©rifier que l'API est bien d√©marr√©e
docker compose ps

# V√©rifier les logs de l'API
docker compose logs api
```

### R√©initialiser compl√®tement

```bash
# Tout supprimer et recommencer
docker compose down -v
docker compose up -d --build
```

## üìù Auteur

Projet r√©alis√© dans le cadre du module **Data Pipeline** - Epitech 2025-2026

## üìÑ License

MIT
